{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Recall the data processing routines from the last lab course. The following excercises build on top of the extracted feature representations, but instead of the prebuilt classifier, we want to implement logistic regression by hand. To this end, make sure, that the variables `train`, `test`, `train_data_features` and `test_data_features` are loaded to your IPython shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#%% download nltk stopwords\n",
    "# import nltk\n",
    "# ntlk.download('stopwords')\n",
    "\n",
    "# load stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# function for preprocessing the data\n",
    "def review_prepro(data, remove_stopwords=False):\n",
    "    # remove HTML tags\n",
    "    review_text = BeautifulSoup(data, 'lxml').get_text()\n",
    "    # remove non-letters and numbers\n",
    "    letters_only = re.sub( '[^a-zA-Z]',\n",
    "                          ' ',\n",
    "                          review_text )\n",
    "    # make all characters lower case and split the documents into single words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # remove stop words\n",
    "        meaningful_words = [ w for w in words if not w in stops ]\n",
    "        # return concatenated single string\n",
    "        return ' '.join(meaningful_words)\n",
    "    else:\n",
    "        # or don't and concatenate to single string\n",
    "        return ' '.join(words)\n",
    "\n",
    "# load data as pandas dataframe\n",
    "train = pd.read_csv('labeledTrainData.tsv', \n",
    "                    header=0,\n",
    "                    delimiter=\"\\t\", \n",
    "                    quoting=3 )\n",
    "\n",
    "test = pd.read_csv('labeledTestData.tsv', \n",
    "                   header=0,\n",
    "                   delimiter=\"\\t\",\n",
    "                   quoting=3 )\n",
    "\n",
    "\n",
    "# preprocess train and test data\n",
    "num_reviews = train['review'].size\n",
    "\n",
    "clean_train_reviews = []\n",
    "for i in range(num_reviews):\n",
    "  #  if (i+1)%1000 == 0:\n",
    "    #    print('Review {} of {}\\n'.format(i+1, num_reviews))\n",
    "    clean_train_reviews.append( review_prepro(train['review'][i], remove_stopwords=True) )\n",
    "    \n",
    "num_test_reviews = test['review'].size\n",
    "\n",
    "clean_test_reviews = []\n",
    "for i in range(num_test_reviews):\n",
    "    #if (i+1)%1000 == 0:\n",
    "     #   print('Review {} of {}\\n'.format(i+1, num_test_reviews))\n",
    "    clean_test_reviews.append( review_prepro(test['review'][i], remove_stopwords=True) )\n",
    "    \n",
    "\n",
    "#%% create BoW\n",
    "# Documentation:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = stops,\n",
    "                             max_features = 5000)\n",
    "\n",
    "# fit the vectorizer and return transformed reviews\n",
    "vectorizer.fit(clean_train_reviews)\n",
    "train_data_features = vectorizer.transform(clean_train_reviews)\n",
    "clean_train_reviews=None \n",
    "# convert to numpy array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "# create BoW representation of test data\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "clean_test_reviews=None\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Write a PYTHON function `logistic_gradient` that expects a training set matrix `X_train`, a ground truth label vector `y_train` and a current weight vector `w` as its input and returns the gradient `g` of the negative log-likelihood function of the logistic regression. Refer to the lecture notes for the mathematical definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    z = np.exp(-np.abs(x))\n",
    "    return np.where(x>=0.0,1.0/(1.0+z),z/(1.0+z))\n",
    "\n",
    "def logistic_gradient(X_train,y_train,w,reg=0.0):\n",
    "    # Rows are variables, Columns are samples\n",
    "    g=np.dot(X_train,sigmoid(np.dot(X_train.T, w))-y_train)+reg*w\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b )Write a PYTHON function `logistic_hessian` that expects a training set matrix `X_train` and a current weight vector `w` as its input and returns the Hessian `H` of the negative log-likelihood function of the logistic regression. Refer to the lecture notes for the mathematical definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_hessian(X_train,w, reg=0.0):\n",
    "    S=sigmoid(np.dot(X_train.T,w))\n",
    "    diagB=S*(1.0-S)\n",
    "    XS=X_train*np.expand_dims(diagB,axis=0)\n",
    "    H=np.dot(XS, X_train.T)+reg*np.eye(w.shape[0])\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write a PYTHON function `find_w` that expects a training set matrix `X_train`, a ground truth label vector `y_train`, a fixed step size `h` and a maximum iteration number `max_it` that determines the optimal logistic regression weight vector `w_star` by performing gradient descent via calling `logistic_gradient` in each iteration. Make sure to include the affine offset $w_0$ in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_w(X_train, y_train, max_it, reg=0.0,w_init=None):\n",
    "    X_offs=np.ones((X_train.shape[0]+1,X_train.shape[1]))\n",
    "    X_offs[1:,:]=X_train\n",
    "    w=np.zeros((X_offs.shape[0]))\n",
    "    if w_init is not None:\n",
    "        w=w_init\n",
    "    cost=np.inf\n",
    "    for i in range(max_it):\n",
    "        delta_w=np.linalg.lstsq(logistic_hessian(X_offs, w,reg=reg),logistic_gradient(X_offs, y_train, w,reg=reg))[0]\n",
    "        w-=np.array(delta_w)\n",
    "        S=sigmoid(np.dot(w.T,X_offs))\n",
    "        oldcost=cost\n",
    "        cost=-(np.dot(np.log(np.where(S<np.finfo('float32').eps,np.finfo('float32').eps,S)),y_train)\n",
    "              +np.dot(np.log(np.where((1.0-S)<np.finfo('float32').eps,np.finfo('float32').eps,1.0-S)),1.0-y_train)\n",
    "              -reg*np.sum(w**2))\n",
    "        if cost>=oldcost:\n",
    "            break\n",
    "        w_star=w\n",
    "        print('Iteration:', i+1, ' Current cost:', cost)\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d) Write a function `classify_log` that expects a weight vector `w` and a test set matrix `X_test` and classifies the samples in `X_test` via logistic regression, returning a label vector `y_test`. Test your implementation on `train_data_features` and `test_data_features` with one iteration and with 10 iterations. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Current cost: 6165.48143183\n",
      "AUC score after 1 iteration: 0.906356178126\n",
      "Iteration: 1  Current cost: 6165.48143183\n",
      "Iteration: 2  Current cost: 3961.1406912\n",
      "Iteration: 3  Current cost: 2634.09296401\n",
      "Iteration: 4  Current cost: 1738.31616292\n",
      "Iteration: 5  Current cost: 1054.12358097\n",
      "Iteration: 6  Current cost: 514.98272427\n",
      "Iteration: 7  Current cost: 196.676386247\n",
      "Iteration: 8  Current cost: 71.7497222914\n",
      "Iteration: 9  Current cost: 26.299656854\n",
      "Iteration: 10  Current cost: 9.67265985876\n",
      "AUC score after 10 iteration: 0.875910041024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "def classify_log(w, X_test):\n",
    "    w0=w[0]\n",
    "    y_test=sigmoid(w0+np.dot(X_test.T,w[1:]))\n",
    "    return y_test\n",
    "\n",
    "#Testing implementation\n",
    "y_train=train['sentiment'].values\n",
    "y_test=test['sentiment'].values\n",
    "train=None\n",
    "test=None\n",
    "w=find_w(train_data_features.T,y_train,1,reg=0)\n",
    "y_pred=classify_log(w,test_data_features.T)\n",
    "auc = AUC( y_test, y_pred )\n",
    "print('AUC score after 1 iteration:',auc)\n",
    "w=find_w(train_data_features.T,y_train,10,reg=0)\n",
    "y_pred=classify_log(w,test_data_features.T)\n",
    "auc = AUC( y_test, y_pred )\n",
    "print('AUC score after 10 iteration:',auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Logistic regression is prone to *overfitting*. To prevent this, regularizing parameters can be used. Adjust your implementation in such a way that instead of minimizing $L(\\mathbf{w})$, it minimizes the term\n",
    "    \t\\begin{equation}\n",
    "    \tL(\\mathbf{w})+\\alpha\\|\\mathbf{w}\\|^2,\n",
    "    \t\\end{equation}\n",
    "    \twhere $\\alpha$ is a non-negative regularization parameter. Test your implementation with $\\alpha=1$ with one iteration and with 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Current cost: 6304.09530912\n",
      "AUC score after 1 iteration: 0.910314236004\n",
      "Iteration: 1  Current cost: 6304.09530912\n",
      "Iteration: 2  Current cost: 4330.85401281\n",
      "Iteration: 3  Current cost: 3437.94965462\n",
      "Iteration: 4  Current cost: 3205.4784196\n",
      "AUC score after 10 iteration: 0.92835827158\n"
     ]
    }
   ],
   "source": [
    "w=find_w(train_data_features.T,y_train,1,reg=1)\n",
    "y_pred=classify_log(w,test_data_features.T)\n",
    "auc = AUC( y_test, y_pred )\n",
    "print('AUC score after 1 iteration:',auc)\n",
    "w=find_w(train_data_features.T,y_train,10,reg=1)\n",
    "y_pred=classify_log(w,test_data_features.T)\n",
    "auc = AUC( y_test, y_pred )\n",
    "print('AUC score after 10 iteration:',auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
